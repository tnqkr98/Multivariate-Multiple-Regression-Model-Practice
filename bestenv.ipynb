{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "torch.manual_seed(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    print('cuda index:', torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def remove_outliers(df, column_name, lower, upper):\n",
    "    removed_outliers = df[column_name].between(df[column_name].quantile(lower), df[column_name].quantile(upper))\n",
    "\n",
    "    print(str(df[column_name][removed_outliers].size) + \"/\" + str(sample_data[column_name].size) + \" data points remain.\")\n",
    "\n",
    "    index_names = df[~removed_outliers].index\n",
    "    return df.drop(index_names)\n",
    "\n",
    "\n",
    "def evaluateRegressor(true, predicted, message=\"    Test Set\"):\n",
    "    MSE = mean_squared_error(true, predicted, squared=True)\n",
    "    MAE = mean_absolute_error(true, predicted)\n",
    "    RMSE = mean_squared_error(true, predicted, squared=False)\n",
    "    R_squared = r2_score(true, predicted)\n",
    "\n",
    "    print(message)\n",
    "    print(\"MSE :\", MSE)\n",
    "    print(\"MAE :\", MAE)\n",
    "    print(\"RMSE :\", RMSE)\n",
    "    print(\"R-Squared :\", R_squared)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112556/124615 data points remain.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 111440 entries, 0 to 124614\n",
      "Data columns (total 20 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   co2         111440 non-null  int64  \n",
      " 1   temp        111440 non-null  float64\n",
      " 2   humid       111440 non-null  float64\n",
      " 3   csd         111440 non-null  int64  \n",
      " 4   sound       111440 non-null  int64  \n",
      " 5   state       111440 non-null  object \n",
      " 6   age         111440 non-null  int64  \n",
      " 7   gender      111440 non-null  object \n",
      " 8   height      111440 non-null  int64  \n",
      " 9   weight      111440 non-null  int64  \n",
      " 10  disease     111440 non-null  object \n",
      " 11  depressive  111440 non-null  int64  \n",
      " 12  disorder    111440 non-null  object \n",
      " 13  media       111440 non-null  int64  \n",
      " 14  liquor      111440 non-null  int64  \n",
      " 15  smoke       111440 non-null  int64  \n",
      " 16  caffeine    111440 non-null  int64  \n",
      " 17  exercise    111440 non-null  int64  \n",
      " 18  stress      111440 non-null  int64  \n",
      " 19  nap         111440 non-null  object \n",
      "dtypes: float64(2), int64(13), object(5)\n",
      "memory usage: 17.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_data = pd.read_csv(\"sample.csv\")\n",
    "sample_data = remove_outliers(sample_data, \"co2\", 0.05, 0.95)\n",
    "idx_zero_temp = sample_data[sample_data['temp'] == 0].index\n",
    "sample_data = sample_data.drop(idx_zero_temp)\n",
    "idx_zero_temp = sample_data[sample_data['stress'] == -1].index\n",
    "sample_data = sample_data.drop(idx_zero_temp)\n",
    "sample_data.info(verbose=True, show_counts=True)\n",
    "\n",
    "sample_data = pd.get_dummies(sample_data)                       # Embedding\n",
    "x_data = sample_data.iloc[:, 5:]\n",
    "y_data = sample_data.iloc[:, [0, 1, 2, 3, 4]]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "# x_data.info(verbose=True, show_counts=True)\n",
    "\n",
    "learning_rate = 0.15\n",
    "iteration_number = 30000\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_scaled = sc.fit_transform(x_train)\n",
    "x_test_scaled = sc.transform(x_test)\n",
    "\n",
    "y_train_scaled = sc.fit_transform(y_train)\n",
    "y_test_scaled = sc.transform(y_test)\n",
    "\n",
    "x_train_scaled = np.array(x_train_scaled, dtype=np.float32)\n",
    "y_train_scaled = np.array(y_train_scaled, dtype=np.float32)\n",
    "x_test_scaled = np.array(x_test_scaled, dtype=np.float32)\n",
    "y_test_scaled = np.array(y_test_scaled, dtype=np.float32)\n",
    "\n",
    "inputs = torch.from_numpy(x_train_scaled)\n",
    "targets = torch.from_numpy(y_train_scaled)\n",
    "\n",
    "test_inputs = torch.from_numpy(x_test_scaled)\n",
    "test_targets = torch.from_numpy(y_test_scaled)\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "mse = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0, trian loss : 1.016886, test loss : 1.011315 \n",
      "epoch  50, trian loss : 0.997370, test loss : 1.003726 \n",
      "epoch 100, trian loss : 0.993362, test loss : 0.999735 \n",
      "epoch 150, trian loss : 0.988680, test loss : 0.995072 \n",
      "epoch 200, trian loss : 0.982815, test loss : 0.989232 \n",
      "epoch 250, trian loss : 0.975177, test loss : 0.981626 \n",
      "epoch 300, trian loss : 0.965228, test loss : 0.971715 \n",
      "epoch 350, trian loss : 0.952898, test loss : 0.959427 \n",
      "epoch 400, trian loss : 0.939126, test loss : 0.945687 \n",
      "epoch 450, trian loss : 0.925556, test loss : 0.932103 \n",
      "epoch 500, trian loss : 0.913058, test loss : 0.919516 \n",
      "epoch 550, trian loss : 0.901144, test loss : 0.907451 \n",
      "epoch 600, trian loss : 0.889009, test loss : 0.895137 \n",
      "epoch 650, trian loss : 0.876331, test loss : 0.882278 \n",
      "epoch 700, trian loss : 0.863273, test loss : 0.869046 \n",
      "epoch 750, trian loss : 0.850244, test loss : 0.855854 \n",
      "epoch 800, trian loss : 0.837726, test loss : 0.843181 \n",
      "epoch 850, trian loss : 0.826169, test loss : 0.831474 \n",
      "epoch 900, trian loss : 0.815883, test loss : 0.821042 \n",
      "epoch 950, trian loss : 0.806969, test loss : 0.811990 \n",
      "epoch 1000, trian loss : 0.799346, test loss : 0.804240 \n",
      "epoch 1050, trian loss : 0.792840, test loss : 0.797626 \n",
      "epoch 1100, trian loss : 0.787269, test loss : 0.791966 \n",
      "epoch 1150, trian loss : 0.782474, test loss : 0.787099 \n",
      "epoch 1200, trian loss : 0.778322, test loss : 0.782889 \n",
      "epoch 1250, trian loss : 0.774701, test loss : 0.779218 \n",
      "epoch 1300, trian loss : 0.771510, test loss : 0.775985 \n",
      "epoch 1350, trian loss : 0.768661, test loss : 0.773098 \n",
      "epoch 1400, trian loss : 0.766080, test loss : 0.770479 \n",
      "epoch 1450, trian loss : 0.763702, test loss : 0.768065 \n",
      "epoch 1500, trian loss : 0.761479, test loss : 0.765804 \n",
      "epoch 1550, trian loss : 0.759367, test loss : 0.763655 \n",
      "epoch 1600, trian loss : 0.757337, test loss : 0.761584 \n",
      "epoch 1650, trian loss : 0.755363, test loss : 0.759569 \n",
      "epoch 1700, trian loss : 0.753425, test loss : 0.757588 \n",
      "epoch 1750, trian loss : 0.751509, test loss : 0.755626 \n",
      "epoch 1800, trian loss : 0.749602, test loss : 0.753673 \n",
      "epoch 1850, trian loss : 0.747695, test loss : 0.751717 \n",
      "epoch 1900, trian loss : 0.745780, test loss : 0.749752 \n",
      "epoch 1950, trian loss : 0.743850, test loss : 0.747772 \n",
      "epoch 2000, trian loss : 0.741900, test loss : 0.745771 \n",
      "epoch 2050, trian loss : 0.739927, test loss : 0.743744 \n",
      "epoch 2100, trian loss : 0.737924, test loss : 0.741689 \n",
      "epoch 2150, trian loss : 0.735891, test loss : 0.739601 \n",
      "epoch 2200, trian loss : 0.733823, test loss : 0.737480 \n",
      "epoch 2250, trian loss : 0.731718, test loss : 0.735322 \n",
      "epoch 2300, trian loss : 0.729577, test loss : 0.733127 \n",
      "epoch 2350, trian loss : 0.727397, test loss : 0.730894 \n",
      "epoch 2400, trian loss : 0.725179, test loss : 0.728623 \n",
      "epoch 2450, trian loss : 0.722924, test loss : 0.726316 \n",
      "epoch 2500, trian loss : 0.720634, test loss : 0.723976 \n",
      "epoch 2550, trian loss : 0.718313, test loss : 0.721604 \n",
      "epoch 2600, trian loss : 0.715963, test loss : 0.719206 \n",
      "epoch 2650, trian loss : 0.713592, test loss : 0.716787 \n",
      "epoch 2700, trian loss : 0.711206, test loss : 0.714354 \n",
      "epoch 2750, trian loss : 0.708812, test loss : 0.711915 \n",
      "epoch 2800, trian loss : 0.706418, test loss : 0.709478 \n",
      "epoch 2850, trian loss : 0.704034, test loss : 0.707052 \n",
      "epoch 2900, trian loss : 0.701667, test loss : 0.704645 \n",
      "epoch 2950, trian loss : 0.699327, test loss : 0.702266 \n",
      "epoch 3000, trian loss : 0.697021, test loss : 0.699922 \n",
      "epoch 3050, trian loss : 0.694755, test loss : 0.697619 \n",
      "epoch 3100, trian loss : 0.692533, test loss : 0.695362 \n",
      "epoch 3150, trian loss : 0.690359, test loss : 0.693153 \n",
      "epoch 3200, trian loss : 0.688235, test loss : 0.690995 \n",
      "epoch 3250, trian loss : 0.686161, test loss : 0.688887 \n",
      "epoch 3300, trian loss : 0.684135, test loss : 0.686829 \n",
      "epoch 3350, trian loss : 0.682157, test loss : 0.684818 \n",
      "epoch 3400, trian loss : 0.680223, test loss : 0.682851 \n",
      "epoch 3450, trian loss : 0.678331, test loss : 0.680926 \n",
      "epoch 3500, trian loss : 0.676476, test loss : 0.679038 \n",
      "epoch 3550, trian loss : 0.674656, test loss : 0.677184 \n",
      "epoch 3600, trian loss : 0.672865, test loss : 0.675360 \n",
      "epoch 3650, trian loss : 0.671101, test loss : 0.673562 \n",
      "epoch 3700, trian loss : 0.669359, test loss : 0.671787 \n",
      "epoch 3750, trian loss : 0.667637, test loss : 0.670030 \n",
      "epoch 3800, trian loss : 0.665930, test loss : 0.668289 \n",
      "epoch 3850, trian loss : 0.664234, test loss : 0.666561 \n",
      "epoch 3900, trian loss : 0.662547, test loss : 0.664840 \n",
      "epoch 3950, trian loss : 0.660865, test loss : 0.663125 \n",
      "epoch 4000, trian loss : 0.659184, test loss : 0.661412 \n",
      "epoch 4050, trian loss : 0.657501, test loss : 0.659697 \n",
      "epoch 4100, trian loss : 0.655813, test loss : 0.657977 \n",
      "epoch 4150, trian loss : 0.654115, test loss : 0.656248 \n",
      "epoch 4200, trian loss : 0.652405, test loss : 0.654507 \n",
      "epoch 4250, trian loss : 0.650679, test loss : 0.652750 \n",
      "epoch 4300, trian loss : 0.648935, test loss : 0.650976 \n",
      "epoch 4350, trian loss : 0.647170, test loss : 0.649180 \n",
      "epoch 4400, trian loss : 0.645381, test loss : 0.647359 \n",
      "epoch 4450, trian loss : 0.643564, test loss : 0.645511 \n",
      "epoch 4500, trian loss : 0.641718, test loss : 0.643634 \n",
      "epoch 4550, trian loss : 0.639840, test loss : 0.641723 \n",
      "epoch 4600, trian loss : 0.637926, test loss : 0.639777 \n",
      "epoch 4650, trian loss : 0.635976, test loss : 0.637793 \n",
      "epoch 4700, trian loss : 0.633985, test loss : 0.635768 \n",
      "epoch 4750, trian loss : 0.631952, test loss : 0.633700 \n",
      "epoch 4800, trian loss : 0.629874, test loss : 0.631585 \n",
      "epoch 4850, trian loss : 0.627749, test loss : 0.629422 \n",
      "epoch 4900, trian loss : 0.625574, test loss : 0.627208 \n",
      "epoch 4950, trian loss : 0.623346, test loss : 0.624940 \n",
      "epoch 5000, trian loss : 0.621065, test loss : 0.622615 \n",
      "epoch 5050, trian loss : 0.618726, test loss : 0.620232 \n",
      "epoch 5100, trian loss : 0.616328, test loss : 0.617788 \n",
      "epoch 5150, trian loss : 0.613868, test loss : 0.615281 \n",
      "epoch 5200, trian loss : 0.611346, test loss : 0.612708 \n",
      "epoch 5250, trian loss : 0.608758, test loss : 0.610067 \n",
      "epoch 5300, trian loss : 0.606103, test loss : 0.607358 \n",
      "epoch 5350, trian loss : 0.603380, test loss : 0.604578 \n",
      "epoch 5400, trian loss : 0.600588, test loss : 0.601727 \n",
      "epoch 5450, trian loss : 0.597726, test loss : 0.598803 \n",
      "epoch 5500, trian loss : 0.594794, test loss : 0.595807 \n",
      "epoch 5550, trian loss : 0.591793, test loss : 0.592740 \n",
      "epoch 5600, trian loss : 0.588724, test loss : 0.589603 \n",
      "epoch 5650, trian loss : 0.585590, test loss : 0.586397 \n",
      "epoch 5700, trian loss : 0.582393, test loss : 0.583127 \n",
      "epoch 5750, trian loss : 0.579137, test loss : 0.579797 \n",
      "epoch 5800, trian loss : 0.575828, test loss : 0.576412 \n",
      "epoch 5850, trian loss : 0.572472, test loss : 0.572979 \n",
      "epoch 5900, trian loss : 0.569076, test loss : 0.569504 \n",
      "epoch 5950, trian loss : 0.565648, test loss : 0.565996 \n",
      "epoch 6000, trian loss : 0.562197, test loss : 0.562465 \n",
      "epoch 6050, trian loss : 0.558733, test loss : 0.558920 \n",
      "epoch 6100, trian loss : 0.555265, test loss : 0.555371 \n",
      "epoch 6150, trian loss : 0.551803, test loss : 0.551828 \n",
      "epoch 6200, trian loss : 0.548356, test loss : 0.548302 \n",
      "epoch 6250, trian loss : 0.544935, test loss : 0.544802 \n",
      "epoch 6300, trian loss : 0.541549, test loss : 0.541337 \n",
      "epoch 6350, trian loss : 0.538205, test loss : 0.537916 \n",
      "epoch 6400, trian loss : 0.534910, test loss : 0.534545 \n",
      "epoch 6450, trian loss : 0.531671, test loss : 0.531233 \n",
      "epoch 6500, trian loss : 0.528494, test loss : 0.527985 \n",
      "epoch 6550, trian loss : 0.525383, test loss : 0.524804 \n",
      "epoch 6600, trian loss : 0.522341, test loss : 0.521696 \n",
      "epoch 6650, trian loss : 0.519371, test loss : 0.518662 \n",
      "epoch 6700, trian loss : 0.516474, test loss : 0.515706 \n",
      "epoch 6750, trian loss : 0.513652, test loss : 0.512828 \n",
      "epoch 6800, trian loss : 0.510906, test loss : 0.510029 \n",
      "epoch 6850, trian loss : 0.508233, test loss : 0.507309 \n",
      "epoch 6900, trian loss : 0.505635, test loss : 0.504667 \n",
      "epoch 6950, trian loss : 0.503109, test loss : 0.502103 \n",
      "epoch 7000, trian loss : 0.500653, test loss : 0.499614 \n",
      "epoch 7050, trian loss : 0.498266, test loss : 0.497197 \n",
      "epoch 7100, trian loss : 0.495944, test loss : 0.494852 \n",
      "epoch 7150, trian loss : 0.493684, test loss : 0.492574 \n",
      "epoch 7200, trian loss : 0.491484, test loss : 0.490360 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7250, trian loss : 0.489341, test loss : 0.488207 \n",
      "epoch 7300, trian loss : 0.487251, test loss : 0.486113 \n",
      "epoch 7350, trian loss : 0.485212, test loss : 0.484073 \n",
      "epoch 7400, trian loss : 0.483219, test loss : 0.482084 \n",
      "epoch 7450, trian loss : 0.481270, test loss : 0.480143 \n",
      "epoch 7500, trian loss : 0.479362, test loss : 0.478247 \n",
      "epoch 7550, trian loss : 0.477493, test loss : 0.476393 \n",
      "epoch 7600, trian loss : 0.475659, test loss : 0.474578 \n",
      "epoch 7650, trian loss : 0.473859, test loss : 0.472799 \n",
      "epoch 7700, trian loss : 0.472089, test loss : 0.471055 \n",
      "epoch 7750, trian loss : 0.470348, test loss : 0.469342 \n",
      "epoch 7800, trian loss : 0.468634, test loss : 0.467658 \n",
      "epoch 7850, trian loss : 0.466945, test loss : 0.466002 \n",
      "epoch 7900, trian loss : 0.465280, test loss : 0.464372 \n",
      "epoch 7950, trian loss : 0.463637, test loss : 0.462767 \n",
      "epoch 8000, trian loss : 0.462015, test loss : 0.461184 \n",
      "epoch 8050, trian loss : 0.460413, test loss : 0.459623 \n",
      "epoch 8100, trian loss : 0.458830, test loss : 0.458082 \n",
      "epoch 8150, trian loss : 0.457266, test loss : 0.456561 \n",
      "epoch 8200, trian loss : 0.455719, test loss : 0.455060 \n",
      "epoch 8250, trian loss : 0.454190, test loss : 0.453576 \n",
      "epoch 8300, trian loss : 0.452677, test loss : 0.452110 \n",
      "epoch 8350, trian loss : 0.451181, test loss : 0.450662 \n",
      "epoch 8400, trian loss : 0.449701, test loss : 0.449230 \n",
      "epoch 8450, trian loss : 0.448237, test loss : 0.447815 \n",
      "epoch 8500, trian loss : 0.446789, test loss : 0.446417 \n",
      "epoch 8550, trian loss : 0.445357, test loss : 0.445034 \n",
      "epoch 8600, trian loss : 0.443941, test loss : 0.443668 \n",
      "epoch 8650, trian loss : 0.442541, test loss : 0.442318 \n",
      "epoch 8700, trian loss : 0.441156, test loss : 0.440984 \n",
      "epoch 8750, trian loss : 0.439788, test loss : 0.439665 \n",
      "epoch 8800, trian loss : 0.438435, test loss : 0.438363 \n",
      "epoch 8850, trian loss : 0.437099, test loss : 0.437076 \n",
      "epoch 8900, trian loss : 0.435779, test loss : 0.435805 \n",
      "epoch 8950, trian loss : 0.434474, test loss : 0.434550 \n",
      "epoch 9000, trian loss : 0.433186, test loss : 0.433310 \n",
      "epoch 9050, trian loss : 0.431913, test loss : 0.432086 \n",
      "epoch 9100, trian loss : 0.430657, test loss : 0.430878 \n",
      "epoch 9150, trian loss : 0.429417, test loss : 0.429685 \n",
      "epoch 9200, trian loss : 0.428192, test loss : 0.428508 \n",
      "epoch 9250, trian loss : 0.426984, test loss : 0.427346 \n",
      "epoch 9300, trian loss : 0.425791, test loss : 0.426199 \n",
      "epoch 9350, trian loss : 0.424613, test loss : 0.425068 \n",
      "epoch 9400, trian loss : 0.423451, test loss : 0.423951 \n",
      "epoch 9450, trian loss : 0.422304, test loss : 0.422849 \n",
      "epoch 9500, trian loss : 0.421173, test loss : 0.421762 \n",
      "epoch 9550, trian loss : 0.420056, test loss : 0.420688 \n",
      "epoch 9600, trian loss : 0.418953, test loss : 0.419629 \n",
      "epoch 9650, trian loss : 0.417865, test loss : 0.418584 \n",
      "epoch 9700, trian loss : 0.416791, test loss : 0.417552 \n",
      "epoch 9750, trian loss : 0.415731, test loss : 0.416534 \n",
      "epoch 9800, trian loss : 0.414684, test loss : 0.415529 \n",
      "epoch 9850, trian loss : 0.413651, test loss : 0.414536 \n",
      "epoch 9900, trian loss : 0.412630, test loss : 0.413556 \n",
      "epoch 9950, trian loss : 0.411623, test loss : 0.412588 \n",
      "epoch 10000, trian loss : 0.410627, test loss : 0.411633 \n",
      "epoch 10050, trian loss : 0.409644, test loss : 0.410688 \n",
      "epoch 10100, trian loss : 0.408673, test loss : 0.409756 \n",
      "epoch 10150, trian loss : 0.407713, test loss : 0.408834 \n",
      "epoch 10200, trian loss : 0.406765, test loss : 0.407924 \n",
      "epoch 10250, trian loss : 0.405827, test loss : 0.407024 \n",
      "epoch 10300, trian loss : 0.404901, test loss : 0.406134 \n",
      "epoch 10350, trian loss : 0.403985, test loss : 0.405255 \n",
      "epoch 10400, trian loss : 0.403080, test loss : 0.404385 \n",
      "epoch 10450, trian loss : 0.402184, test loss : 0.403525 \n",
      "epoch 10500, trian loss : 0.401299, test loss : 0.402675 \n",
      "epoch 10550, trian loss : 0.400423, test loss : 0.401834 \n",
      "epoch 10600, trian loss : 0.399556, test loss : 0.401002 \n",
      "epoch 10650, trian loss : 0.398699, test loss : 0.400178 \n",
      "epoch 10700, trian loss : 0.397851, test loss : 0.399363 \n",
      "epoch 10750, trian loss : 0.397012, test loss : 0.398553 \n",
      "epoch 10800, trian loss : 0.396281, test loss : 0.397822 \n",
      "epoch 10850, trian loss : 0.421000, test loss : 0.424681 \n",
      "epoch 10900, trian loss : 0.404795, test loss : 0.405119 \n",
      "epoch 10950, trian loss : 0.394135, test loss : 0.395734 \n",
      "epoch 11000, trian loss : 0.393197, test loss : 0.394869 \n",
      "epoch 11050, trian loss : 0.392384, test loss : 0.394092 \n",
      "epoch 11100, trian loss : 0.391588, test loss : 0.393328 \n",
      "epoch 11150, trian loss : 0.390805, test loss : 0.392573 \n",
      "epoch 11200, trian loss : 0.390043, test loss : 0.391830 \n",
      "epoch 11250, trian loss : 0.389540, test loss : 0.391320 \n",
      "epoch 11300, trian loss : 0.398984, test loss : 0.401215 \n",
      "epoch 11350, trian loss : 0.412808, test loss : 0.412991 \n",
      "epoch 11400, trian loss : 0.388401, test loss : 0.390113 \n",
      "epoch 11450, trian loss : 0.386626, test loss : 0.388504 \n",
      "epoch 11500, trian loss : 0.385804, test loss : 0.387725 \n",
      "epoch 11550, trian loss : 0.385137, test loss : 0.387075 \n",
      "epoch 11600, trian loss : 0.385087, test loss : 0.387015 \n",
      "epoch 11650, trian loss : 0.393054, test loss : 0.395183 \n",
      "epoch 11700, trian loss : 0.403405, test loss : 0.404497 \n",
      "epoch 11750, trian loss : 0.385062, test loss : 0.386839 \n",
      "epoch 11800, trian loss : 0.382282, test loss : 0.384271 \n",
      "epoch 11850, trian loss : 0.381466, test loss : 0.383500 \n",
      "epoch 11900, trian loss : 0.381646, test loss : 0.383682 \n",
      "epoch 11950, trian loss : 0.387052, test loss : 0.389151 \n",
      "epoch 12000, trian loss : 0.395127, test loss : 0.396755 \n",
      "epoch 12050, trian loss : 0.383046, test loss : 0.384873 \n",
      "epoch 12100, trian loss : 0.379261, test loss : 0.381318 \n",
      "epoch 12150, trian loss : 0.378438, test loss : 0.380551 \n",
      "epoch 12200, trian loss : 0.379566, test loss : 0.381690 \n",
      "epoch 12250, trian loss : 0.385545, test loss : 0.387650 \n",
      "epoch 12300, trian loss : 0.385491, test loss : 0.387312 \n",
      "epoch 12350, trian loss : 0.378583, test loss : 0.380603 \n",
      "epoch 12400, trian loss : 0.376193, test loss : 0.378345 \n",
      "epoch 12450, trian loss : 0.376086, test loss : 0.378275 \n",
      "epoch 12500, trian loss : 0.378592, test loss : 0.380783 \n",
      "epoch 12550, trian loss : 0.381823, test loss : 0.383886 \n",
      "epoch 12600, trian loss : 0.378045, test loss : 0.380083 \n",
      "epoch 12650, trian loss : 0.374493, test loss : 0.376667 \n",
      "epoch 12700, trian loss : 0.373476, test loss : 0.375715 \n",
      "epoch 12750, trian loss : 0.374418, test loss : 0.376670 \n",
      "epoch 12800, trian loss : 0.376772, test loss : 0.378979 \n",
      "epoch 12850, trian loss : 0.376032, test loss : 0.378166 \n",
      "epoch 12900, trian loss : 0.372883, test loss : 0.375088 \n",
      "epoch 12950, trian loss : 0.371234, test loss : 0.373512 \n",
      "epoch 13000, trian loss : 0.371267, test loss : 0.373573 \n",
      "epoch 13050, trian loss : 0.372566, test loss : 0.374857 \n",
      "epoch 13100, trian loss : 0.372939, test loss : 0.375179 \n",
      "epoch 13150, trian loss : 0.370965, test loss : 0.373218 \n",
      "epoch 13200, trian loss : 0.369146, test loss : 0.371459 \n",
      "epoch 13250, trian loss : 0.368593, test loss : 0.370941 \n",
      "epoch 13300, trian loss : 0.369119, test loss : 0.371468 \n",
      "epoch 13350, trian loss : 0.369621, test loss : 0.371942 \n",
      "epoch 13400, trian loss : 0.368646, test loss : 0.370957 \n",
      "epoch 13450, trian loss : 0.367065, test loss : 0.369412 \n",
      "epoch 13500, trian loss : 0.366182, test loss : 0.368564 \n",
      "epoch 13550, trian loss : 0.366171, test loss : 0.368563 \n",
      "epoch 13600, trian loss : 0.366474, test loss : 0.368853 \n",
      "epoch 13650, trian loss : 0.366064, test loss : 0.368430 \n",
      "epoch 13700, trian loss : 0.364902, test loss : 0.367283 \n",
      "epoch 13750, trian loss : 0.363920, test loss : 0.366329 \n",
      "epoch 13800, trian loss : 0.363553, test loss : 0.365977 \n",
      "epoch 13850, trian loss : 0.363593, test loss : 0.366014 \n",
      "epoch 13900, trian loss : 0.363406, test loss : 0.365816 \n",
      "epoch 13950, trian loss : 0.362637, test loss : 0.365050 \n",
      "epoch 14000, trian loss : 0.361728, test loss : 0.364159 \n",
      "epoch 14050, trian loss : 0.361160, test loss : 0.363606 \n",
      "epoch 14100, trian loss : 0.360965, test loss : 0.363415 \n",
      "epoch 14150, trian loss : 0.360802, test loss : 0.363245 \n",
      "epoch 14200, trian loss : 0.360304, test loss : 0.362745 \n",
      "epoch 14250, trian loss : 0.359556, test loss : 0.362006 \n",
      "epoch 14300, trian loss : 0.358917, test loss : 0.361378 \n",
      "epoch 14350, trian loss : 0.358550, test loss : 0.361018 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14400, trian loss : 0.358321, test loss : 0.360787 \n",
      "epoch 14450, trian loss : 0.357963, test loss : 0.360425 \n",
      "epoch 14500, trian loss : 0.357384, test loss : 0.359849 \n",
      "epoch 14550, trian loss : 0.356767, test loss : 0.359239 \n",
      "epoch 14600, trian loss : 0.356301, test loss : 0.358779 \n",
      "epoch 14650, trian loss : 0.355986, test loss : 0.358464 \n",
      "epoch 14700, trian loss : 0.355669, test loss : 0.358145 \n",
      "epoch 14750, trian loss : 0.355218, test loss : 0.357693 \n",
      "epoch 14800, trian loss : 0.354673, test loss : 0.357151 \n",
      "epoch 14850, trian loss : 0.354174, test loss : 0.356657 \n",
      "epoch 14900, trian loss : 0.353789, test loss : 0.356273 \n",
      "epoch 14950, trian loss : 0.353459, test loss : 0.355942 \n",
      "epoch 15000, trian loss : 0.353080, test loss : 0.355560 \n",
      "epoch 15050, trian loss : 0.352616, test loss : 0.355096 \n",
      "epoch 15100, trian loss : 0.352134, test loss : 0.354616 \n",
      "epoch 15150, trian loss : 0.351710, test loss : 0.354193 \n",
      "epoch 15200, trian loss : 0.351350, test loss : 0.353833 \n",
      "epoch 15250, trian loss : 0.350996, test loss : 0.353476 \n",
      "epoch 15300, trian loss : 0.350594, test loss : 0.353072 \n",
      "epoch 15350, trian loss : 0.350152, test loss : 0.352630 \n",
      "epoch 15400, trian loss : 0.349723, test loss : 0.352202 \n",
      "epoch 15450, trian loss : 0.349340, test loss : 0.351817 \n",
      "epoch 15500, trian loss : 0.348985, test loss : 0.351460 \n",
      "epoch 15550, trian loss : 0.348618, test loss : 0.351090 \n",
      "epoch 15600, trian loss : 0.348218, test loss : 0.350689 \n",
      "epoch 15650, trian loss : 0.347807, test loss : 0.350276 \n",
      "epoch 15700, trian loss : 0.347415, test loss : 0.349884 \n",
      "epoch 15750, trian loss : 0.347052, test loss : 0.349518 \n",
      "epoch 15800, trian loss : 0.346698, test loss : 0.349162 \n",
      "epoch 15850, trian loss : 0.346330, test loss : 0.348790 \n",
      "epoch 15900, trian loss : 0.345945, test loss : 0.348403 \n",
      "epoch 15950, trian loss : 0.345561, test loss : 0.348017 \n",
      "epoch 16000, trian loss : 0.345194, test loss : 0.347647 \n",
      "epoch 16050, trian loss : 0.344843, test loss : 0.347293 \n",
      "epoch 16100, trian loss : 0.344492, test loss : 0.346939 \n",
      "epoch 16150, trian loss : 0.344131, test loss : 0.346574 \n",
      "epoch 16200, trian loss : 0.343763, test loss : 0.346204 \n",
      "epoch 16250, trian loss : 0.343400, test loss : 0.345839 \n",
      "epoch 16300, trian loss : 0.343050, test loss : 0.345485 \n",
      "epoch 16350, trian loss : 0.342708, test loss : 0.345140 \n",
      "epoch 16400, trian loss : 0.342364, test loss : 0.344791 \n",
      "epoch 16450, trian loss : 0.342013, test loss : 0.344438 \n",
      "epoch 16500, trian loss : 0.341662, test loss : 0.344083 \n",
      "epoch 16550, trian loss : 0.341316, test loss : 0.343733 \n",
      "epoch 16600, trian loss : 0.340979, test loss : 0.343392 \n",
      "epoch 16650, trian loss : 0.340646, test loss : 0.343055 \n",
      "epoch 16700, trian loss : 0.340310, test loss : 0.342715 \n",
      "epoch 16750, trian loss : 0.339972, test loss : 0.342373 \n",
      "epoch 16800, trian loss : 0.339634, test loss : 0.342032 \n",
      "epoch 16850, trian loss : 0.339302, test loss : 0.341696 \n",
      "epoch 16900, trian loss : 0.338976, test loss : 0.341365 \n",
      "epoch 16950, trian loss : 0.338651, test loss : 0.341036 \n",
      "epoch 17000, trian loss : 0.338325, test loss : 0.340705 \n",
      "epoch 17050, trian loss : 0.337998, test loss : 0.340375 \n",
      "epoch 17100, trian loss : 0.337674, test loss : 0.340046 \n",
      "epoch 17150, trian loss : 0.337353, test loss : 0.339721 \n",
      "epoch 17200, trian loss : 0.337036, test loss : 0.339399 \n",
      "epoch 17250, trian loss : 0.336721, test loss : 0.339079 \n",
      "epoch 17300, trian loss : 0.336405, test loss : 0.338758 \n",
      "epoch 17350, trian loss : 0.336089, test loss : 0.338438 \n",
      "epoch 17400, trian loss : 0.335776, test loss : 0.338120 \n",
      "epoch 17450, trian loss : 0.335465, test loss : 0.337805 \n",
      "epoch 17500, trian loss : 0.335158, test loss : 0.337493 \n",
      "epoch 17550, trian loss : 0.334851, test loss : 0.337181 \n",
      "epoch 17600, trian loss : 0.334545, test loss : 0.336870 \n",
      "epoch 17650, trian loss : 0.334239, test loss : 0.336560 \n",
      "epoch 17700, trian loss : 0.333936, test loss : 0.336252 \n",
      "epoch 17750, trian loss : 0.333636, test loss : 0.335947 \n",
      "epoch 17800, trian loss : 0.333337, test loss : 0.335643 \n",
      "epoch 17850, trian loss : 0.333039, test loss : 0.335340 \n",
      "epoch 17900, trian loss : 0.332742, test loss : 0.335037 \n",
      "epoch 17950, trian loss : 0.332446, test loss : 0.334737 \n",
      "epoch 18000, trian loss : 0.332153, test loss : 0.334438 \n",
      "epoch 18050, trian loss : 0.331861, test loss : 0.334142 \n",
      "epoch 18100, trian loss : 0.331571, test loss : 0.333846 \n",
      "epoch 18150, trian loss : 0.331282, test loss : 0.333551 \n",
      "epoch 18200, trian loss : 0.330994, test loss : 0.333258 \n",
      "epoch 18250, trian loss : 0.330707, test loss : 0.332966 \n",
      "epoch 18300, trian loss : 0.330423, test loss : 0.332677 \n",
      "epoch 18350, trian loss : 0.330140, test loss : 0.332388 \n",
      "epoch 18400, trian loss : 0.329858, test loss : 0.332101 \n",
      "epoch 18450, trian loss : 0.329577, test loss : 0.331814 \n",
      "epoch 18500, trian loss : 0.329298, test loss : 0.331530 \n",
      "epoch 18550, trian loss : 0.329020, test loss : 0.331246 \n",
      "epoch 18600, trian loss : 0.328744, test loss : 0.330965 \n",
      "epoch 18650, trian loss : 0.328469, test loss : 0.330684 \n",
      "epoch 18700, trian loss : 0.328195, test loss : 0.330405 \n",
      "epoch 18750, trian loss : 0.327922, test loss : 0.330127 \n",
      "epoch 18800, trian loss : 0.327651, test loss : 0.329850 \n",
      "epoch 18850, trian loss : 0.327381, test loss : 0.329574 \n",
      "epoch 18900, trian loss : 0.327113, test loss : 0.329301 \n",
      "epoch 18950, trian loss : 0.326846, test loss : 0.329028 \n",
      "epoch 19000, trian loss : 0.326580, test loss : 0.328756 \n",
      "epoch 19050, trian loss : 0.326315, test loss : 0.328486 \n",
      "epoch 19100, trian loss : 0.326052, test loss : 0.328217 \n",
      "epoch 19150, trian loss : 0.325790, test loss : 0.327949 \n",
      "epoch 19200, trian loss : 0.325530, test loss : 0.327683 \n",
      "epoch 19250, trian loss : 0.325270, test loss : 0.327417 \n",
      "epoch 19300, trian loss : 0.325012, test loss : 0.327153 \n",
      "epoch 19350, trian loss : 0.324755, test loss : 0.326890 \n",
      "epoch 19400, trian loss : 0.324499, test loss : 0.326628 \n",
      "epoch 19450, trian loss : 0.324244, test loss : 0.326368 \n",
      "epoch 19500, trian loss : 0.323991, test loss : 0.326109 \n",
      "epoch 19550, trian loss : 0.323738, test loss : 0.325850 \n",
      "epoch 19600, trian loss : 0.323487, test loss : 0.325593 \n",
      "epoch 19650, trian loss : 0.323237, test loss : 0.325338 \n",
      "epoch 19700, trian loss : 0.322989, test loss : 0.325083 \n",
      "epoch 19750, trian loss : 0.322741, test loss : 0.324829 \n",
      "epoch 19800, trian loss : 0.322494, test loss : 0.324577 \n",
      "epoch 19850, trian loss : 0.322249, test loss : 0.324325 \n",
      "epoch 19900, trian loss : 0.322004, test loss : 0.324075 \n",
      "epoch 19950, trian loss : 0.321761, test loss : 0.323826 \n",
      "epoch 20000, trian loss : 0.321519, test loss : 0.323578 \n",
      "epoch 20050, trian loss : 0.321278, test loss : 0.323331 \n",
      "epoch 20100, trian loss : 0.321038, test loss : 0.323085 \n",
      "epoch 20150, trian loss : 0.320799, test loss : 0.322840 \n",
      "epoch 20200, trian loss : 0.320562, test loss : 0.322597 \n",
      "epoch 20250, trian loss : 0.320325, test loss : 0.322354 \n",
      "epoch 20300, trian loss : 0.320089, test loss : 0.322113 \n",
      "epoch 20350, trian loss : 0.319855, test loss : 0.321872 \n",
      "epoch 20400, trian loss : 0.319621, test loss : 0.321632 \n",
      "epoch 20450, trian loss : 0.319389, test loss : 0.321394 \n",
      "epoch 20500, trian loss : 0.319157, test loss : 0.321157 \n",
      "epoch 20550, trian loss : 0.318927, test loss : 0.320920 \n",
      "epoch 20600, trian loss : 0.318697, test loss : 0.320685 \n",
      "epoch 20650, trian loss : 0.318468, test loss : 0.320450 \n",
      "epoch 20700, trian loss : 0.318240, test loss : 0.320217 \n",
      "epoch 20750, trian loss : 0.318014, test loss : 0.319984 \n",
      "epoch 20800, trian loss : 0.317788, test loss : 0.319753 \n",
      "epoch 20850, trian loss : 0.317564, test loss : 0.319522 \n",
      "epoch 20900, trian loss : 0.317340, test loss : 0.319293 \n",
      "epoch 20950, trian loss : 0.317117, test loss : 0.319064 \n",
      "epoch 21000, trian loss : 0.316895, test loss : 0.318837 \n",
      "epoch 21050, trian loss : 0.316674, test loss : 0.318610 \n",
      "epoch 21100, trian loss : 0.316454, test loss : 0.318384 \n",
      "epoch 21150, trian loss : 0.316235, test loss : 0.318159 \n",
      "epoch 21200, trian loss : 0.316016, test loss : 0.317935 \n",
      "epoch 21250, trian loss : 0.315799, test loss : 0.317713 \n",
      "epoch 21300, trian loss : 0.315583, test loss : 0.317491 \n",
      "epoch 21350, trian loss : 0.315367, test loss : 0.317269 \n",
      "epoch 21400, trian loss : 0.315152, test loss : 0.317049 \n",
      "epoch 21450, trian loss : 0.314939, test loss : 0.316830 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21500, trian loss : 0.314726, test loss : 0.316611 \n",
      "epoch 21550, trian loss : 0.314513, test loss : 0.316394 \n",
      "epoch 21600, trian loss : 0.314302, test loss : 0.316177 \n",
      "epoch 21650, trian loss : 0.314092, test loss : 0.315961 \n",
      "epoch 21700, trian loss : 0.313882, test loss : 0.315746 \n",
      "epoch 21750, trian loss : 0.313673, test loss : 0.315532 \n",
      "epoch 21800, trian loss : 0.313465, test loss : 0.315318 \n",
      "epoch 21850, trian loss : 0.313258, test loss : 0.315106 \n",
      "epoch 21900, trian loss : 0.313051, test loss : 0.314894 \n",
      "epoch 21950, trian loss : 0.312846, test loss : 0.314684 \n",
      "epoch 22000, trian loss : 0.312641, test loss : 0.314474 \n",
      "epoch 22050, trian loss : 0.312437, test loss : 0.314264 \n",
      "epoch 22100, trian loss : 0.312233, test loss : 0.314056 \n",
      "epoch 22150, trian loss : 0.312030, test loss : 0.313848 \n",
      "epoch 22200, trian loss : 0.311829, test loss : 0.313641 \n",
      "epoch 22250, trian loss : 0.311627, test loss : 0.313434 \n",
      "epoch 22300, trian loss : 0.311426, test loss : 0.313228 \n",
      "epoch 22350, trian loss : 0.311224, test loss : 0.313020 \n",
      "epoch 22400, trian loss : 0.311020, test loss : 0.312810 \n",
      "epoch 22450, trian loss : 0.310815, test loss : 0.312597 \n",
      "epoch 22500, trian loss : 0.310699, test loss : 0.312470 \n",
      "epoch 22550, trian loss : 0.312131, test loss : 0.313975 \n",
      "epoch 22600, trian loss : 0.324302, test loss : 0.326551 \n",
      "epoch 22650, trian loss : 0.313923, test loss : 0.316061 \n",
      "epoch 22700, trian loss : 0.309010, test loss : 0.311558 \n",
      "epoch 22750, trian loss : 0.310550, test loss : 0.313335 \n",
      "epoch 22800, trian loss : 0.312275, test loss : 0.315096 \n",
      "epoch 22850, trian loss : 0.310010, test loss : 0.312671 \n",
      "epoch 22900, trian loss : 0.307999, test loss : 0.310582 \n",
      "epoch 22950, trian loss : 0.307552, test loss : 0.310144 \n",
      "epoch 23000, trian loss : 0.308301, test loss : 0.310976 \n",
      "epoch 23050, trian loss : 0.309607, test loss : 0.312359 \n",
      "epoch 23100, trian loss : 0.309569, test loss : 0.312281 \n",
      "epoch 23150, trian loss : 0.308274, test loss : 0.310905 \n",
      "epoch 23200, trian loss : 0.307420, test loss : 0.310011 \n",
      "epoch 23250, trian loss : 0.307368, test loss : 0.309963 \n",
      "epoch 23300, trian loss : 0.307770, test loss : 0.310386 \n",
      "epoch 23350, trian loss : 0.307887, test loss : 0.310507 \n",
      "epoch 23400, trian loss : 0.307513, test loss : 0.310159 \n",
      "epoch 23450, trian loss : 0.308930, test loss : 0.311784 \n",
      "epoch 23500, trian loss : 0.315800, test loss : 0.318615 \n",
      "epoch 23550, trian loss : 0.309884, test loss : 0.312125 \n",
      "epoch 23600, trian loss : 0.305127, test loss : 0.307239 \n",
      "epoch 23650, trian loss : 0.304642, test loss : 0.306628 \n",
      "epoch 23700, trian loss : 0.308010, test loss : 0.309766 \n",
      "epoch 23750, trian loss : 0.312943, test loss : 0.314233 \n",
      "epoch 23800, trian loss : 0.306772, test loss : 0.308273 \n",
      "epoch 23850, trian loss : 0.303688, test loss : 0.305445 \n",
      "epoch 23900, trian loss : 0.303294, test loss : 0.305095 \n",
      "epoch 23950, trian loss : 0.304386, test loss : 0.306130 \n",
      "epoch 24000, trian loss : 0.306345, test loss : 0.307964 \n",
      "epoch 24050, trian loss : 0.305997, test loss : 0.307551 \n",
      "epoch 24100, trian loss : 0.305015, test loss : 0.306646 \n",
      "epoch 24150, trian loss : 0.308572, test loss : 0.310249 \n",
      "epoch 24200, trian loss : 0.310339, test loss : 0.311882 \n",
      "epoch 24250, trian loss : 0.303386, test loss : 0.305327 \n",
      "epoch 24300, trian loss : 0.301215, test loss : 0.303365 \n",
      "epoch 24350, trian loss : 0.301792, test loss : 0.304210 \n",
      "epoch 24400, trian loss : 0.310837, test loss : 0.313977 \n",
      "epoch 24450, trian loss : 0.309017, test loss : 0.311649 \n",
      "epoch 24500, trian loss : 0.303001, test loss : 0.305504 \n",
      "epoch 24550, trian loss : 0.302808, test loss : 0.305283 \n",
      "epoch 24600, trian loss : 0.303423, test loss : 0.305923 \n",
      "epoch 24650, trian loss : 0.303370, test loss : 0.305901 \n",
      "epoch 24700, trian loss : 0.303231, test loss : 0.305846 \n",
      "epoch 24750, trian loss : 0.304210, test loss : 0.306941 \n",
      "epoch 24800, trian loss : 0.304597, test loss : 0.307267 \n",
      "epoch 24850, trian loss : 0.302715, test loss : 0.305227 \n",
      "epoch 24900, trian loss : 0.301235, test loss : 0.303647 \n",
      "epoch 24950, trian loss : 0.300734, test loss : 0.303083 \n",
      "epoch 25000, trian loss : 0.300777, test loss : 0.303056 \n",
      "epoch 25050, trian loss : 0.301356, test loss : 0.303484 \n",
      "epoch 25100, trian loss : 0.306919, test loss : 0.308646 \n",
      "epoch 25150, trian loss : 0.315488, test loss : 0.316019 \n",
      "epoch 25200, trian loss : 0.300697, test loss : 0.302172 \n",
      "epoch 25250, trian loss : 0.298177, test loss : 0.299936 \n",
      "epoch 25300, trian loss : 0.298051, test loss : 0.299829 \n",
      "epoch 25350, trian loss : 0.299321, test loss : 0.301015 \n",
      "epoch 25400, trian loss : 0.302589, test loss : 0.304093 \n",
      "epoch 25450, trian loss : 0.303047, test loss : 0.304391 \n",
      "epoch 25500, trian loss : 0.301463, test loss : 0.302941 \n",
      "epoch 25550, trian loss : 0.302163, test loss : 0.303739 \n",
      "epoch 25600, trian loss : 0.301425, test loss : 0.303080 \n",
      "epoch 25650, trian loss : 0.298801, test loss : 0.300610 \n",
      "epoch 25700, trian loss : 0.297311, test loss : 0.299196 \n",
      "epoch 25750, trian loss : 0.296937, test loss : 0.298788 \n",
      "epoch 25800, trian loss : 0.299934, test loss : 0.301605 \n",
      "epoch 25850, trian loss : 0.315121, test loss : 0.315910 \n",
      "epoch 25900, trian loss : 0.298658, test loss : 0.300111 \n",
      "epoch 25950, trian loss : 0.296009, test loss : 0.297811 \n",
      "epoch 26000, trian loss : 0.296065, test loss : 0.297950 \n",
      "epoch 26050, trian loss : 0.297446, test loss : 0.299395 \n",
      "epoch 26100, trian loss : 0.299447, test loss : 0.301507 \n",
      "epoch 26150, trian loss : 0.300420, test loss : 0.302919 \n",
      "epoch 26200, trian loss : 0.311201, test loss : 0.314668 \n",
      "epoch 26250, trian loss : 0.300665, test loss : 0.303105 \n",
      "epoch 26300, trian loss : 0.295406, test loss : 0.297697 \n",
      "epoch 26350, trian loss : 0.294763, test loss : 0.297035 \n",
      "epoch 26400, trian loss : 0.295434, test loss : 0.297821 \n",
      "epoch 26450, trian loss : 0.298231, test loss : 0.300879 \n",
      "epoch 26500, trian loss : 0.300721, test loss : 0.303441 \n",
      "epoch 26550, trian loss : 0.299002, test loss : 0.301612 \n",
      "epoch 26600, trian loss : 0.299039, test loss : 0.301571 \n",
      "epoch 26650, trian loss : 0.298516, test loss : 0.300818 \n",
      "epoch 26700, trian loss : 0.295896, test loss : 0.298010 \n",
      "epoch 26750, trian loss : 0.294299, test loss : 0.296335 \n",
      "epoch 26800, trian loss : 0.293835, test loss : 0.295796 \n",
      "epoch 26850, trian loss : 0.295563, test loss : 0.297360 \n",
      "epoch 26900, trian loss : 0.311551, test loss : 0.312743 \n",
      "epoch 26950, trian loss : 0.299263, test loss : 0.300314 \n",
      "epoch 27000, trian loss : 0.295149, test loss : 0.296701 \n",
      "epoch 27050, trian loss : 0.294643, test loss : 0.296272 \n",
      "epoch 27100, trian loss : 0.294962, test loss : 0.296549 \n",
      "epoch 27150, trian loss : 0.296060, test loss : 0.297524 \n",
      "epoch 27200, trian loss : 0.297675, test loss : 0.298984 \n",
      "epoch 27250, trian loss : 0.297375, test loss : 0.298650 \n",
      "epoch 27300, trian loss : 0.295448, test loss : 0.296855 \n",
      "epoch 27350, trian loss : 0.294384, test loss : 0.295909 \n",
      "epoch 27400, trian loss : 0.294118, test loss : 0.295707 \n",
      "epoch 27450, trian loss : 0.293949, test loss : 0.295580 \n",
      "epoch 27500, trian loss : 0.293473, test loss : 0.295149 \n",
      "epoch 27550, trian loss : 0.292859, test loss : 0.294598 \n",
      "epoch 27600, trian loss : 0.292608, test loss : 0.294493 \n",
      "epoch 27650, trian loss : 0.299126, test loss : 0.302241 \n",
      "epoch 27700, trian loss : 0.311991, test loss : 0.314599 \n",
      "epoch 27750, trian loss : 0.291811, test loss : 0.294033 \n",
      "epoch 27800, trian loss : 0.290333, test loss : 0.292478 \n",
      "epoch 27850, trian loss : 0.290484, test loss : 0.292688 \n",
      "epoch 27900, trian loss : 0.292410, test loss : 0.294862 \n",
      "epoch 27950, trian loss : 0.297245, test loss : 0.299979 \n",
      "epoch 28000, trian loss : 0.295807, test loss : 0.298360 \n",
      "epoch 28050, trian loss : 0.293723, test loss : 0.296195 \n",
      "epoch 28100, trian loss : 0.294615, test loss : 0.297053 \n",
      "epoch 28150, trian loss : 0.293934, test loss : 0.296194 \n",
      "epoch 28200, trian loss : 0.291599, test loss : 0.293752 \n",
      "epoch 28250, trian loss : 0.290595, test loss : 0.292814 \n",
      "epoch 28300, trian loss : 0.293350, test loss : 0.296046 \n",
      "epoch 28350, trian loss : 0.302738, test loss : 0.305535 \n",
      "epoch 28400, trian loss : 0.292566, test loss : 0.294810 \n",
      "epoch 28450, trian loss : 0.289528, test loss : 0.291656 \n",
      "epoch 28500, trian loss : 0.289246, test loss : 0.291346 \n",
      "epoch 28550, trian loss : 0.290010, test loss : 0.292128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28600, trian loss : 0.291304, test loss : 0.293403 \n",
      "epoch 28650, trian loss : 0.291226, test loss : 0.293189 \n",
      "epoch 28700, trian loss : 0.292116, test loss : 0.293808 \n",
      "epoch 28750, trian loss : 0.317226, test loss : 0.317131 \n",
      "epoch 28800, trian loss : 0.291066, test loss : 0.292237 \n",
      "epoch 28850, trian loss : 0.287697, test loss : 0.289306 \n",
      "epoch 28900, trian loss : 0.287514, test loss : 0.289150 \n",
      "epoch 28950, trian loss : 0.288743, test loss : 0.290284 \n",
      "epoch 29000, trian loss : 0.293587, test loss : 0.294889 \n",
      "epoch 29050, trian loss : 0.294505, test loss : 0.295519 \n",
      "epoch 29100, trian loss : 0.291152, test loss : 0.292415 \n",
      "epoch 29150, trian loss : 0.291508, test loss : 0.292902 \n",
      "epoch 29200, trian loss : 0.291783, test loss : 0.293215 \n",
      "epoch 29250, trian loss : 0.289508, test loss : 0.291059 \n",
      "epoch 29300, trian loss : 0.287880, test loss : 0.289492 \n",
      "epoch 29350, trian loss : 0.288320, test loss : 0.289828 \n",
      "epoch 29400, trian loss : 0.299790, test loss : 0.301005 \n",
      "epoch 29450, trian loss : 0.293725, test loss : 0.294611 \n",
      "epoch 29500, trian loss : 0.287058, test loss : 0.288564 \n",
      "epoch 29550, trian loss : 0.286281, test loss : 0.287914 \n",
      "epoch 29600, trian loss : 0.286768, test loss : 0.288416 \n",
      "epoch 29650, trian loss : 0.288311, test loss : 0.289918 \n",
      "epoch 29700, trian loss : 0.289325, test loss : 0.290800 \n",
      "epoch 29750, trian loss : 0.291064, test loss : 0.292394 \n",
      "epoch 29800, trian loss : 0.307545, test loss : 0.307706 \n",
      "epoch 29850, trian loss : 0.290062, test loss : 0.291192 \n",
      "epoch 29900, trian loss : 0.285424, test loss : 0.287028 \n",
      "epoch 29950, trian loss : 0.284762, test loss : 0.286413 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "for iteration in range(iteration_number):\n",
    "    optimizer.zero_grad()\n",
    "    results = model(inputs)\n",
    "    loss = mse(results, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_list.append(loss.data)\n",
    "\n",
    "    test_results = model(test_inputs)\n",
    "    test_loss = mse(test_results, test_targets)\n",
    "    test_loss_list.append(test_loss.data)\n",
    "\n",
    "    if iteration % 50 == 0:\n",
    "        print('epoch %3d, trian loss : %f, test loss : %f ' % (iteration, loss.data, test_loss.data))\n",
    "\n",
    "\n",
    "plt.plot(range(iteration_number), loss_list, range(iteration_number), test_loss_list)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "input_x_test = torch.from_numpy(x_test_scaled)\n",
    "predicted = model(input_x_test.float()).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test Set\n",
      "MSE : 0.3799472\n",
      "MAE : 0.40624523\n",
      "RMSE : 0.6122048\n",
      "R-Squared : 0.6226670779408663\n",
      "    Valid Set\n",
      "MSE : 69970.68676122156\n",
      "MAE : 91.69348440674702\n",
      "RMSE : 124.54921852721732\n",
      "R-Squared : 0.6226670777739932\n"
     ]
    }
   ],
   "source": [
    "predict_valid_y = model(input_x_test.float()).data.numpy()\n",
    "evaluateRegressor(test_targets, predict_valid_y)\n",
    "predict_valid_y = sc.inverse_transform(predict_valid_y)\n",
    "evaluateRegressor(y_test, predict_valid_y,\"    Valid Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
